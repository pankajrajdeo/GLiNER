{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##GLiNER"
      ],
      "metadata": {
        "id": "rQsbJ_4jvwDG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dbZ1bb0TPl6X"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/urchade/GLiNER.git\n",
        "%cd GLiNER\n",
        "\n",
        "# Optional: If you want to install in \"editable\" mode\n",
        "!pip install -r requirements.txt\n",
        "# OR simply:\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "CMaBtpsJvtP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --config configs/config.yaml --log_dir models/bioformer_ner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLyCGBYvVyit",
        "outputId": "0ff1b1cc-39b2-40d7-af95-6db39349486f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-10 01:03:57.909326: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744247037.930339   10154 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744247037.936744   10154 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-10 01:03:57.959047: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Dataset size: 9998\n",
            "Dataset is shuffled...\n",
            "Dataset is splitted...\n",
            "Initializing cross fuser...\n",
            "Post fusion layer: l2l-l2t-t2t\n",
            "Number of post fusion layers: 3\n",
            "Training for 1 epochs (562 steps)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "  0% 0/562 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "  0% 1/562 [00:09<1:24:59,  9.09s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 629 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "  1% 5/562 [00:31<55:02,  5.93s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 532 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "  2% 10/562 [01:10<1:06:25,  7.22s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 557 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "  2% 11/562 [01:18<1:08:31,  7.46s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 539 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "  3% 19/562 [02:17<1:11:13,  7.87s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 518 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "  4% 22/562 [02:38<1:06:48,  7.42s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 553 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "  4% 25/562 [03:07<1:19:00,  8.83s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 869 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "  8% 44/562 [05:28<1:06:43,  7.73s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 527 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "  8% 46/562 [05:45<1:08:06,  7.92s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 639 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "  9% 51/562 [06:19<1:01:01,  7.17s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 524 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 15% 85/562 [10:40<54:27,  6.85s/it]  /content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 605 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 18% 100/562 [12:47<1:01:21,  7.97s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 619 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 522 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 19% 105/562 [13:23<56:33,  7.43s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 541 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 21% 117/562 [15:09<1:11:11,  9.60s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 521 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 23% 129/562 [16:50<58:27,  8.10s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 575 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 26% 144/562 [18:52<54:57,  7.89s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 668 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 30% 167/562 [22:19<55:03,  8.36s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 584 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 30% 171/562 [22:55<56:18,  8.64s/it]  /content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 516 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 32% 182/562 [24:32<55:12,  8.72s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 655 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 34% 190/562 [25:38<51:22,  8.29s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 519 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 36% 200/562 [27:06<57:54,  9.60s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 558 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 692 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 38% 214/562 [29:06<52:58,  9.13s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 528 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 39% 217/562 [29:32<49:12,  8.56s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 616 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 41% 231/562 [31:40<48:40,  8.82s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 594 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 43% 244/562 [33:35<44:13,  8.35s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 803 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 44% 250/562 [34:41<55:23, 10.65s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 524 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 48% 271/562 [37:43<45:12,  9.32s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 598 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 51% 285/562 [39:44<43:30,  9.42s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 627 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 52% 294/562 [41:08<39:06,  8.76s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 589 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 56% 313/562 [43:49<37:37,  9.07s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 523 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 59% 332/562 [46:27<37:16,  9.72s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 539 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 63% 352/562 [49:37<35:52, 10.25s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 618 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 65% 367/562 [52:01<31:41,  9.75s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 572 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 66% 372/562 [52:40<25:28,  8.04s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 534 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 75% 419/562 [59:43<21:11,  8.89s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 527 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 75% 420/562 [59:51<20:29,  8.66s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 582 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 76% 428/562 [1:01:08<21:19,  9.55s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 526 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 84% 470/562 [1:07:31<12:02,  7.86s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 789 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 87% 490/562 [1:10:35<11:50,  9.86s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 700 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 88% 493/562 [1:11:00<10:33,  9.17s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 557 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "{'loss': 7329.6155, 'grad_norm': 2942.27392578125, 'learning_rate': 1.1019730189165578e-06, 'epoch': 0.89}\n",
            " 92% 518/562 [1:14:43<06:55,  9.44s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 516 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 94% 530/562 [1:16:35<05:11,  9.74s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 526 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            " 95% 535/562 [1:17:25<04:23,  9.76s/it]/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 513 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "100% 562/562 [1:21:28<00:00, 11.17s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 563 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 717 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 631 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "\n",
            "  0% 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/63 [00:00<00:19,  3.09it/s]\u001b[A\n",
            "  5% 3/63 [00:01<00:24,  2.44it/s]\u001b[A\n",
            "  6% 4/63 [00:01<00:26,  2.27it/s]\u001b[A\n",
            "  8% 5/63 [00:02<00:28,  2.03it/s]\u001b[A\n",
            " 10% 6/63 [00:02<00:30,  1.87it/s]\u001b[A/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 576 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "\n",
            " 11% 7/63 [00:03<00:33,  1.68it/s]\u001b[A\n",
            " 13% 8/63 [00:04<00:34,  1.58it/s]\u001b[A\n",
            " 14% 9/63 [00:04<00:34,  1.59it/s]\u001b[A\n",
            " 16% 10/63 [00:05<00:32,  1.64it/s]\u001b[A\n",
            " 17% 11/63 [00:06<00:31,  1.64it/s]\u001b[A\n",
            " 19% 12/63 [00:06<00:29,  1.71it/s]\u001b[A\n",
            " 21% 13/63 [00:07<00:27,  1.85it/s]\u001b[A\n",
            " 22% 14/63 [00:07<00:26,  1.82it/s]\u001b[A\n",
            " 24% 15/63 [00:08<00:26,  1.82it/s]\u001b[A\n",
            " 25% 16/63 [00:09<00:29,  1.58it/s]\u001b[A\n",
            " 27% 17/63 [00:09<00:28,  1.61it/s]\u001b[A\n",
            " 29% 18/63 [00:10<00:26,  1.67it/s]\u001b[A\n",
            " 30% 19/63 [00:10<00:26,  1.67it/s]\u001b[A\n",
            " 32% 20/63 [00:11<00:25,  1.71it/s]\u001b[A\n",
            " 33% 21/63 [00:11<00:24,  1.69it/s]\u001b[A\n",
            " 35% 22/63 [00:12<00:22,  1.84it/s]\u001b[A\n",
            " 37% 23/63 [00:12<00:22,  1.79it/s]\u001b[A\n",
            " 38% 24/63 [00:13<00:21,  1.80it/s]\u001b[A\n",
            " 40% 25/63 [00:14<00:20,  1.85it/s]\u001b[A\n",
            " 41% 26/63 [00:14<00:19,  1.89it/s]\u001b[A\n",
            " 43% 27/63 [00:15<00:19,  1.84it/s]\u001b[A\n",
            " 44% 28/63 [00:15<00:19,  1.82it/s]\u001b[A\n",
            " 46% 29/63 [00:16<00:17,  1.95it/s]\u001b[A/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 539 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "\n",
            " 48% 30/63 [00:16<00:18,  1.82it/s]\u001b[A\n",
            " 49% 31/63 [00:17<00:17,  1.81it/s]\u001b[A\n",
            " 51% 32/63 [00:17<00:17,  1.75it/s]\u001b[A\n",
            " 52% 33/63 [00:18<00:18,  1.63it/s]\u001b[A/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 546 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "\n",
            " 54% 34/63 [00:19<00:18,  1.61it/s]\u001b[A\n",
            " 56% 35/63 [00:19<00:16,  1.73it/s]\u001b[A\n",
            " 57% 36/63 [00:20<00:15,  1.77it/s]\u001b[A\n",
            " 59% 37/63 [00:20<00:14,  1.75it/s]\u001b[A\n",
            " 60% 38/63 [00:21<00:14,  1.72it/s]\u001b[A\n",
            " 62% 39/63 [00:22<00:13,  1.73it/s]\u001b[A\n",
            " 63% 40/63 [00:22<00:13,  1.75it/s]\u001b[A\n",
            " 65% 41/63 [00:23<00:12,  1.75it/s]\u001b[A/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 514 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 526 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "\n",
            " 67% 42/63 [00:23<00:12,  1.74it/s]\u001b[A\n",
            " 68% 43/63 [00:24<00:11,  1.79it/s]\u001b[A\n",
            " 70% 44/63 [00:24<00:09,  1.93it/s]\u001b[A\n",
            " 71% 45/63 [00:25<00:09,  1.86it/s]\u001b[A\n",
            " 73% 46/63 [00:25<00:09,  1.82it/s]\u001b[A\n",
            " 75% 47/63 [00:26<00:09,  1.74it/s]\u001b[A\n",
            " 76% 48/63 [00:27<00:08,  1.69it/s]\u001b[A/content/GLiNER/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 529 has been truncated to 512\n",
            "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
            "\n",
            " 78% 49/63 [00:27<00:07,  1.75it/s]\u001b[A\n",
            " 79% 50/63 [00:28<00:07,  1.80it/s]\u001b[A\n",
            " 81% 51/63 [00:28<00:06,  1.88it/s]\u001b[A\n",
            " 83% 52/63 [00:29<00:05,  1.99it/s]\u001b[A\n",
            " 84% 53/63 [00:29<00:04,  2.03it/s]\u001b[A\n",
            " 86% 54/63 [00:29<00:04,  2.10it/s]\u001b[A\n",
            " 87% 55/63 [00:30<00:03,  2.07it/s]\u001b[A\n",
            " 89% 56/63 [00:30<00:03,  2.17it/s]\u001b[A\n",
            " 90% 57/63 [00:31<00:02,  2.13it/s]\u001b[A\n",
            " 92% 58/63 [00:31<00:02,  2.00it/s]\u001b[A\n",
            " 94% 59/63 [00:32<00:01,  2.02it/s]\u001b[A\n",
            " 95% 60/63 [00:32<00:01,  2.10it/s]\u001b[A\n",
            " 97% 61/63 [00:33<00:01,  1.99it/s]\u001b[A\n",
            " 98% 62/63 [00:33<00:00,  1.95it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3180.735107421875, 'eval_runtime': 40.2536, 'eval_samples_per_second': 24.842, 'eval_steps_per_second': 1.565, 'epoch': 1.0}\n",
            "100% 562/562 [1:22:14<00:00, 11.17s/it]\n",
            "100% 63/63 [00:34<00:00,  1.91it/s]\u001b[A\n",
            "{'train_runtime': 4934.0147, 'train_samples_per_second': 1.822, 'train_steps_per_second': 0.114, 'train_loss': 6612.933225366993, 'epoch': 1.0}\n",
            "100% 562/562 [1:22:14<00:00,  8.78s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference"
      ],
      "metadata": {
        "id": "HuDU987kvoZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gliner import GLiNER\n",
        "# Example: if the trainer saved a folder logs/my_bioformer_run/model_1000\n",
        "checkpoint_dir = \"/content/GLiNER/models/bioformer_ner/checkpoint-562\"\n",
        "\n",
        "model = GLiNER.from_pretrained(checkpoint_dir, load_tokenizer=True)\n",
        "model.eval()\n",
        "\n",
        "# Now you can predict\n",
        "text = \"Analysis of peptides and proteins by temperature-responsive chromatographic system using N-isopropylacrylamide polymer-modified columns. A new method of HPLC using packing materials modified with a temperature responsive polymer, poly(N-isopropylacrylamide) (PIPAAm), was developed. Homogeneous PIPAAm polymer and its copolymer with butyl methacrylate (BMA) were synthesized and grafted to aminopropyl silica by activated ester-amine coupling and they were used as packing materials. The surface properties and functions of the stationary phases are controlled by external temperature. Isocratic elution by aqueous mobile phase alone is the basis for separation of peptides and protein. The separation of the mixture of three peptides, insulin chain A and B and beta-endorphin fragment 1-27 was achieved by changing the column temperature with 0.9% NaCl aqueous solution as the sole eluent. Retention of peptides and proteins was controlled both by column temperature and by NaCl concentration in the aqueous mobile phases in this chromatographic system.\"\n",
        "labels = [\n",
        "    \"CONCEPT\",\n",
        "    \"CHEMICAL_ENTITY\",\n",
        "    \"METHOD_OR_TECHNIQUE\",\n",
        "    \"PHYSICAL_PROPERTY\",\n",
        "    \"PHYSICAL_PROPERTY\",\n",
        "    \"GENE_OR_GENE_PRODUCT\"\n",
        "]\n",
        "entities = model.predict_entities(text, labels, threshold=0.3)\n",
        "print(entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8zmhDh-akOA",
        "outputId": "d01194de-32dc-40ba-9617-af56cae84aad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "config.json not found in /content/GLiNER/models/bioformer_ner/checkpoint-562\n",
            "WARNING:huggingface_hub.hub_mixin:config.json not found in /content/GLiNER/models/bioformer_ner/checkpoint-562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing cross fuser...\n",
            "Post fusion layer: l2l-l2t-t2t\n",
            "Number of post fusion layers: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'start': 12, 'end': 20, 'text': 'peptides', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.7445377111434937}, {'start': 25, 'end': 33, 'text': 'proteins', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.6645873188972473}, {'start': 37, 'end': 82, 'text': 'temperature-responsive chromatographic system', 'label': 'METHOD_OR_TECHNIQUE', 'score': 0.7623119950294495}, {'start': 89, 'end': 127, 'text': 'N-isopropylacrylamide polymer-modified', 'label': 'CHEMICAL_ENTITY', 'score': 0.7901886701583862}, {'start': 153, 'end': 157, 'text': 'HPLC', 'label': 'METHOD_OR_TECHNIQUE', 'score': 0.7837425470352173}, {'start': 198, 'end': 228, 'text': 'temperature responsive polymer', 'label': 'CHEMICAL_ENTITY', 'score': 0.38367825746536255}, {'start': 230, 'end': 266, 'text': 'poly(N-isopropylacrylamide) (PIPAAm)', 'label': 'CHEMICAL_ENTITY', 'score': 0.7757289409637451}, {'start': 295, 'end': 309, 'text': 'PIPAAm polymer', 'label': 'CHEMICAL_ENTITY', 'score': 0.8328989148139954}, {'start': 333, 'end': 357, 'text': 'butyl methacrylate (BMA)', 'label': 'CHEMICAL_ENTITY', 'score': 0.7948375344276428}, {'start': 390, 'end': 408, 'text': 'aminopropyl silica', 'label': 'CHEMICAL_ENTITY', 'score': 0.5589168071746826}, {'start': 412, 'end': 442, 'text': 'activated ester-amine coupling', 'label': 'METHOD_OR_TECHNIQUE', 'score': 0.5238481760025024}, {'start': 586, 'end': 603, 'text': 'Isocratic elution', 'label': 'METHOD_OR_TECHNIQUE', 'score': 0.7300388813018799}, {'start': 607, 'end': 627, 'text': 'aqueous mobile phase', 'label': 'CHEMICAL_ENTITY', 'score': 0.42089223861694336}, {'start': 665, 'end': 673, 'text': 'peptides', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.7797463536262512}, {'start': 678, 'end': 685, 'text': 'protein', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.625662624835968}, {'start': 726, 'end': 734, 'text': 'peptides', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.6620292067527771}, {'start': 736, 'end': 757, 'text': 'insulin chain A and B', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.7450922727584839}, {'start': 762, 'end': 790, 'text': 'beta-endorphin fragment 1-27', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.5506654977798462}, {'start': 849, 'end': 870, 'text': 'NaCl aqueous solution', 'label': 'CHEMICAL_ENTITY', 'score': 0.6996932029724121}, {'start': 904, 'end': 912, 'text': 'peptides', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.711881697177887}, {'start': 917, 'end': 925, 'text': 'proteins', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.6291611194610596}, {'start': 975, 'end': 979, 'text': 'NaCl', 'label': 'CHEMICAL_ENTITY', 'score': 0.6325209140777588}, {'start': 1001, 'end': 1022, 'text': 'aqueous mobile phases', 'label': 'CHEMICAL_ENTITY', 'score': 0.4047791063785553}, {'start': 1031, 'end': 1053, 'text': 'chromatographic system', 'label': 'METHOD_OR_TECHNIQUE', 'score': 0.5143930315971375}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradio App"
      ],
      "metadata": {
        "id": "9ZTqC-kLvkvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjYfiZEouEnk",
        "outputId": "abc0022e-c760-4b12-a1fe-47238e9291ab"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy this code to a new cell and run it\n",
        "%%writefile gradio_app_colab.py\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from gliner import GLiNER\n",
        "import re\n",
        "import html\n",
        "import hashlib\n",
        "import sys\n",
        "\n",
        "# Function to generate consistent colors from entity type names\n",
        "def get_entity_color(entity_type):\n",
        "    \"\"\"Generate a consistent color based on the entity type string.\"\"\"\n",
        "    # Use hash of the entity type to generate consistent colors\n",
        "    hash_obj = hashlib.md5(entity_type.encode())\n",
        "    hash_hex = hash_obj.hexdigest()\n",
        "\n",
        "    # Convert first 6 characters of hash to RGB values\n",
        "    r = int(hash_hex[:2], 16)\n",
        "    g = int(hash_hex[2:4], 16)\n",
        "    b = int(hash_hex[4:6], 16)\n",
        "\n",
        "    # Ensure colors are vibrant enough (boost saturation/value)\n",
        "    # Calculate HSV from RGB\n",
        "    max_val = max(r, g, b)\n",
        "    min_val = min(r, g, b)\n",
        "\n",
        "    # Adjust saturation and brightness for readability\n",
        "    if max_val > 0:\n",
        "        # Boost saturation\n",
        "        saturation_boost = 0.7\n",
        "        if max_val != min_val:\n",
        "            r = int(r + (max_val - r) * saturation_boost)\n",
        "            g = int(g + (max_val - g) * saturation_boost)\n",
        "            b = int(b + (max_val - b) * saturation_boost)\n",
        "\n",
        "        # Ensure brightness is appropriate for colored backgrounds with white text\n",
        "        brightness = (0.299 * r + 0.587 * g + 0.114 * b) / 255\n",
        "        if brightness > 0.65:  # If too bright, darken it\n",
        "            r = int(r * 0.8)\n",
        "            g = int(g * 0.8)\n",
        "            b = int(b * 0.8)\n",
        "        elif brightness < 0.2:  # If too dark, lighten it\n",
        "            r = min(255, int(r * 1.5))\n",
        "            g = min(255, int(g * 1.5))\n",
        "            b = min(255, int(b * 1.5))\n",
        "\n",
        "    # Convert back to hex\n",
        "    return f\"#{r:02x}{g:02x}{b:02x}\"\n",
        "\n",
        "# Dictionary to cache colors for entity types\n",
        "entity_color_cache = {}\n",
        "\n",
        "def get_color_for_entity(entity_type):\n",
        "    if entity_type not in entity_color_cache:\n",
        "        entity_color_cache[entity_type] = get_entity_color(entity_type)\n",
        "    return entity_color_cache[entity_type]\n",
        "\n",
        "def load_model(model_path):\n",
        "    print(f\"Loading model from {model_path}...\")\n",
        "    model = GLiNER.from_pretrained(model_path, load_tokenizer=True)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def filter_overlapping_entities(entities):\n",
        "    # Sort entities by score in descending order\n",
        "    sorted_entities = sorted(entities, key=lambda x: x.get(\"score\", 0), reverse=True)\n",
        "\n",
        "    # Keep track of token positions that are already covered\n",
        "    covered_positions = set()\n",
        "    filtered_entities = []\n",
        "\n",
        "    for entity in sorted_entities:\n",
        "        start = entity.get(\"start\") or entity.get(\"token_start\", 0)\n",
        "        end = entity.get(\"end\") or entity.get(\"token_end\", 0)\n",
        "\n",
        "        # Check if this entity overlaps with any already covered position\n",
        "        current_positions = set(range(start, end + 1))\n",
        "        if not current_positions.intersection(covered_positions):\n",
        "            filtered_entities.append(entity)\n",
        "            covered_positions.update(current_positions)\n",
        "\n",
        "    return filtered_entities\n",
        "\n",
        "# The fixed highlight_entities function to match GLiNER's output format\n",
        "def highlight_entities(text, entities):\n",
        "    safe_text = html.escape(text)\n",
        "    words = safe_text.split()\n",
        "\n",
        "    word_entities = {}\n",
        "    for entity in entities:\n",
        "        # Check the entity format and adapt accordingly\n",
        "        start = entity.get(\"start\") or entity.get(\"token_start\")\n",
        "        end = entity.get(\"end\") or entity.get(\"token_end\")\n",
        "        # GLiNER might use 'label' instead of 'type'\n",
        "        entity_type = entity.get(\"type\") or entity.get(\"label\")\n",
        "\n",
        "        if start is None or end is None or entity_type is None:\n",
        "            print(f\"Warning: Skipping entity with invalid format: {entity}\")\n",
        "            continue\n",
        "\n",
        "        if start >= len(words) or end >= len(words):\n",
        "            continue\n",
        "\n",
        "        for i in range(start, end + 1):\n",
        "            if i < len(words):\n",
        "                if i not in word_entities:\n",
        "                    word_entities[i] = []\n",
        "                word_entities[i].append((entity_type, start, end))\n",
        "\n",
        "    # Print the first entity to debug\n",
        "    if entities and len(entities) > 0:\n",
        "        print(\"First entity format:\", entities[0])\n",
        "\n",
        "    # Rest of function remains the same\n",
        "    result = []\n",
        "    for i, word in enumerate(words):\n",
        "        if i in word_entities:\n",
        "            for entity_type, start, end in word_entities[i]:\n",
        "                color = get_color_for_entity(entity_type)\n",
        "                result.append(f'<span style=\"background-color: {color};\">{word}</span>')\n",
        "\n",
        "                if i == end:\n",
        "                    result.append(f' <span style=\"background-color: {color}; color: white; padding: 0px 4px; border-radius: 3px; font-size: 0.8em; font-weight: bold;\">{entity_type}</span> ')\n",
        "                else:\n",
        "                    result.append(' ')\n",
        "                break\n",
        "        else:\n",
        "            result.append(word + ' ')\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "def predict_entities(text, labels_input, threshold, allow_nested):\n",
        "    labels = [label.strip() for label in labels_input.split(\",\")]\n",
        "\n",
        "    # First, print what the entities look like to debug\n",
        "    entities = model.predict_entities(text, labels, threshold=threshold)\n",
        "\n",
        "    # Print sample of the entity format\n",
        "    print(f\"Entity format example: {entities[0] if entities else 'No entities found'}\")\n",
        "\n",
        "    if not allow_nested:\n",
        "        entities = filter_overlapping_entities(entities)\n",
        "\n",
        "    highlighted_html = highlight_entities(text, entities)\n",
        "\n",
        "    entity_list = \"<div style='margin-top: 15px;'>\"\n",
        "    if entities:\n",
        "        entity_list += \"<p><strong>Detected entities:</strong></p><ul>\"\n",
        "        for e in entities:\n",
        "            # Adapt to possible different key names\n",
        "            entity_text = e.get(\"text\", \"\")\n",
        "            entity_type = e.get(\"type\") or e.get(\"label\", \"\")\n",
        "            entity_score = e.get(\"score\", 0.0)\n",
        "\n",
        "            entity_list += f\"<li>{entity_text} - {entity_type} (score: {entity_score:.2f})</li>\"\n",
        "        entity_list += \"</ul>\"\n",
        "    else:\n",
        "        entity_list += \"<p>No entities detected.</p>\"\n",
        "    entity_list += \"</div>\"\n",
        "\n",
        "    return highlighted_html + entity_list\n",
        "\n",
        "example_biomedical_text = \"\"\"Analysis of peptides and proteins by temperature-responsive chromatographic system using N-isopropylacrylamide polymer-modified columns. A new method of HPLC using packing materials modified with a temperature responsive polymer, poly(N-isopropylacrylamide) (PIPAAm), was developed. Homogeneous PIPAAm polymer and its copolymer with butyl methacrylate (BMA) were synthesized and grafted to aminopropyl silica by activated ester-amine coupling and they were used as packing materials.\"\"\"\n",
        "\n",
        "example_biomedical_labels = \"CONCEPT,CHEMICAL_ENTITY,METHOD_OR_TECHNIQUE,PHYSICAL_PROPERTY,GENE_OR_GENE_PRODUCT\"\n",
        "\n",
        "def create_interface(model_path):\n",
        "    global model\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    with gr.Blocks(title=\"NuNER Zero\", css=\"span { display: inline; }\") as demo:\n",
        "        gr.Markdown(\"# NuNER Zero\")\n",
        "\n",
        "        with gr.Accordion(\"How to run this model locally\", open=False):\n",
        "            gr.Markdown(\"Instructions for running the model locally would go here.\")\n",
        "\n",
        "        with gr.Column():\n",
        "            text_input = gr.Textbox(\n",
        "                label=\"Text input\",\n",
        "                placeholder=\"Enter text here...\",\n",
        "                value=example_biomedical_text,\n",
        "                lines=5\n",
        "            )\n",
        "\n",
        "            labels_input = gr.Textbox(\n",
        "                label=\"Labels\",\n",
        "                placeholder=\"Enter comma-separated labels\",\n",
        "                value=example_biomedical_labels,\n",
        "                lines=2\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    threshold = gr.Slider(\n",
        "                        label=\"Threshold\",\n",
        "                        minimum=0.1,\n",
        "                        maximum=0.9,\n",
        "                        value=0.3,\n",
        "                        step=0.05\n",
        "                    )\n",
        "                    gr.Markdown(\"Lower the threshold to increase how many entities get predicted.\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"Allow for nested NER?\")\n",
        "                    nested_ner = gr.Checkbox(label=\"Nested NER\", value=False)\n",
        "\n",
        "            gr.Markdown(\"### Predicted Entities\")\n",
        "            output = gr.HTML()\n",
        "\n",
        "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "            submit_btn.click(\n",
        "                fn=predict_entities,\n",
        "                inputs=[text_input, labels_input, threshold, nested_ner],\n",
        "                outputs=output\n",
        "            )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# For Colab, use a simplified argument handling approach\n",
        "model_path = \"/content/GLiNER/models/bioformer_ner/checkpoint-562\"  # Default path\n",
        "\n",
        "# Create and launch the interface\n",
        "demo = create_interface(model_path)\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H61xkmHVuBZo",
        "outputId": "56fe816b-53dd-431a-8d02-c119cb55fad5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting gradio_app_colab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gradio_app_colab.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7iKJgDIulqX",
        "outputId": "c0f6bc9c-a0d6-4811-f91d-8b3c82bb1574"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/GLiNER/models/bioformer_ner/checkpoint-562...\n",
            "Initializing cross fuser...\n",
            "Post fusion layer: l2l-l2t-t2t\n",
            "Number of post fusion layers: 3\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://1b36bfced39406f476.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Entity format example: {'start': 12, 'end': 20, 'text': 'peptides', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.6525564193725586}\n",
            "First entity format: {'start': 295, 'end': 309, 'text': 'PIPAAm polymer', 'label': 'CHEMICAL_ENTITY', 'score': 0.7739928364753723}\n",
            "Entity format example: {'start': 12, 'end': 20, 'text': 'peptides', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.6525564193725586}\n",
            "First entity format: {'start': 12, 'end': 20, 'text': 'peptides', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.6525564193725586}\n",
            "Entity format example: {'start': 12, 'end': 20, 'text': 'peptides', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.6525564193725586}\n",
            "First entity format: {'start': 12, 'end': 20, 'text': 'peptides', 'label': 'GENE_OR_GENE_PRODUCT', 'score': 0.6525564193725586}\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2997, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GLiNER/gradio_app_colab.py\", line 223, in <module>\n",
            "    demo.launch(share=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2903, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3001, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/http_server.py\", line 69, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1123, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://1b36bfced39406f476.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}