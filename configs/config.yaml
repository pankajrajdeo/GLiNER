# Model Configuration
model_name: bioformers/bioformer-16L # Bioformer for biomedical text
labels_encoder: "sentence-transformers/all-MiniLM-L6-v2" # Label encoder model
name: "biomedical span level gliner"
max_width: 12
hidden_size: 384 # Match bioformer-16L's hidden size
dropout: 0.3
fine_tune: true
subtoken_pooling: first
fuse_layers: false
post_fusion_schema: "l2l-l2t-t2t"
span_mode: markerV0

# Training Parameters
num_steps: -1 # Set to -1 to use num_epochs instead
num_epochs: 1 # Train for 10 epochs
train_batch_size: 16
eval_every: 1 # Evaluate every epoch
warmup_ratio: 0.1
scheduler_type: "cosine"

# loss function
loss_alpha: 0.75
loss_gamma: 0
label_smoothing: 0
loss_reduction: "sum"

# Learning Rate and weight decay Configuration
lr_encoder: 1e-5
lr_others: 3e-5
weight_decay_encoder: 0.1
weight_decay_other: 0.01

max_grad_norm: 10.0

# Directory Paths
root_dir: gliner_logs
train_data: "/content/GLiNER/data/gliner_training_data.json" # Path to your converted data
val_data_dir: "none"

# Pretrained Model Path
# Use "none" if no pretrained model is being used
prev_path: null

save_total_limit: 3 #maximum amount of checkpoints to save

# Advanced Training Settings
size_sup: -1
max_types: 100
shuffle_types: true
random_drop: true
max_neg_type_ratio: 1
max_len: 512
freeze_token_rep: false
